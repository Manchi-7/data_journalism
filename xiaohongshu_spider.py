"""
xiaohongshu_spider.py   â€”â€” å¤šå…³é”®è¯å°çº¢ä¹¦ç¬”è®°çˆ¬è™«ï¼ˆSeleniumç‰ˆï¼‰
-------------------------------------------------------------
åŠŸèƒ½ï¼š
1. å…³é”®è¯åˆ—è¡¨ä» keywords.txt è¯»å–ï¼ˆæ¯è¡Œä¸€ä¸ªå…³é”®è¯ï¼‰
2. æ¯ä¸ªå…³é”®è¯çš„ç¬”è®°å†™å…¥ Excel ç‹¬ç«‹ Sheet
3. ä½¿ç”¨Seleniumæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼Œç»•è¿‡APIé™åˆ¶
4. æ”¯æŒè‡ªåŠ¨æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹

ä½¿ç”¨è¯´æ˜ï¼š
1. éœ€è¦å®‰è£…ï¼špip install selenium pandas openpyxl
2. éœ€è¦ä¸‹è½½ChromeDriverå¹¶é…ç½®è·¯å¾„
3. è¿è¡Œå‰éœ€è¦æ‰‹åŠ¨ç™»å½•å°çº¢ä¹¦ï¼ˆä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼‰

"""

import os, time, random
from pathlib import Path

import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException


with open("keywords.txt", "r", encoding="utf-8") as f:
    KEYWORD_LIST = [line.strip() for line in f if line.strip()]

MAX_POSTS = 100          # æ¯ä¸ªå…³é”®è¯æŠ“å–ç¬”è®°æ•°é‡
SCROLL_TIMES = 15        # é¡µé¢æ»šåŠ¨æ¬¡æ•°ï¼ˆæ¯æ¬¡æ»šåŠ¨åŠ è½½æ›´å¤šï¼‰
CHROMEDRIVER_PATH = "path/to/chromedriver"  # ChromeDriverè·¯å¾„ï¼Œæ”¹ä¸ºä½ çš„å®é™…è·¯å¾„


def init_driver():
    """åˆå§‹åŒ–Chromeæµè§ˆå™¨"""
    options = Options()
    # options.add_argument("--headless")  # æ— å¤´æ¨¡å¼ï¼Œå–æ¶ˆæ³¨é‡Šå¯éšè—æµè§ˆå™¨çª—å£
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--start-maximized")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    
    service = Service(CHROMEDRIVER_PATH)
    driver = webdriver.Chrome(service=service, options=options)
    
    # éšè—webdriverç‰¹å¾
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
    })
    
    return driver


def login_xiaohongshu(driver):
    """æ‰“å¼€å°çº¢ä¹¦å¹¶ç­‰å¾…æ‰‹åŠ¨ç™»å½•"""
    driver.get("https://www.xiaohongshu.com/")
    print("\nğŸ‘‰ è¯·åœ¨æµè§ˆå™¨ä¸­ç™»å½•å°çº¢ä¹¦è´¦å·...")
    print("   ç™»å½•å®Œæˆåå›åˆ°ç»ˆç«¯æŒ‰ Enter ç»§ç»­")
    input()
    
    # éªŒè¯ç™»å½•çŠ¶æ€ - å°è¯•å¤šä¸ªå¯èƒ½çš„é€‰æ‹©å™¨
    login_selectors = [
        ".user-info",
        ".avatar", 
        "[class*='avatar']",
        "[class*='user']",
        "img[alt*='å¤´åƒ']",
        ".login-container",  # å¦‚æœèƒ½æ‰¾åˆ°è¿™ä¸ªè¯´æ˜æœªç™»å½•
    ]
    
    is_logged_in = False
    for selector in login_selectors[:-1]:  # æ’é™¤æœ€åä¸€ä¸ªï¼ˆæœªç™»å½•æ ‡è¯†ï¼‰
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                print(f"âœ… æ£€æµ‹åˆ°ç™»å½•å…ƒç´ : {selector}")
                is_logged_in = True
                break
        except:
            continue
    
    if not is_logged_in:
        # æ£€æŸ¥æ˜¯å¦æœ‰æœªç™»å½•çš„æ ‡è¯†
        try:
            driver.find_element(By.CSS_SELECTOR, ".login-container")
            print("âš ï¸  æ£€æµ‹åˆ°æœªç™»å½•æ ‡è¯†ï¼Œä½†å°†ç»§ç»­å°è¯•...")
        except:
            # æ‰¾ä¸åˆ°æœªç™»å½•æ ‡è¯†ï¼Œå¯èƒ½æ˜¯å·²ç™»å½•
            print("âœ… æœªæ£€æµ‹åˆ°æ˜ç¡®ç™»å½•å…ƒç´ ï¼Œä½†é¡µé¢æ­£å¸¸ï¼Œå°†ç»§ç»­...")
    
    return True  # æ€»æ˜¯è¿”å›Trueç»§ç»­æ‰§è¡Œ


def scroll_to_load_more(driver, times=5):
    """æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ›´å¤šå†…å®¹"""
    for i in range(times):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(random.uniform(2, 3))
        print(f"  ğŸ“œ æ»šåŠ¨ {i+1}/{times}")


def crawl_keyword(driver, keyword: str, max_posts: int) -> pd.DataFrame:
    """çˆ¬å–æŒ‡å®šå…³é”®è¯çš„ç¬”è®°"""
    rows = []
    
    # æ„å»ºæœç´¢URL
    search_url = f"https://www.xiaohongshu.com/search_result?keyword={keyword}&source=web_search_result_notes"
    
    print(f"\næ­£åœ¨çˆ¬å–å…³é”®è¯: '{keyword}'")
    driver.get(search_url)
    time.sleep(3)
    
    # æ»šåŠ¨åŠ è½½æ›´å¤š
    scroll_to_load_more(driver, SCROLL_TIMES)
    
    try:
        # ç­‰å¾…ç¬”è®°åˆ—è¡¨åŠ è½½
        WebDriverWait(driver, 10).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, "section.note-item, .feeds-container a"))
        )
    except TimeoutException:
        print(f"  âš ï¸  æœªæ‰¾åˆ°ç¬”è®°åˆ—è¡¨ï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²å˜åŒ–")
        return pd.DataFrame()
    
    # å°è¯•å¤šç§é€‰æ‹©å™¨
    selectors = [
        "section.note-item",
        ".feeds-container a.cover",
        "a[href*='/explore/']",
        ".note-item",
    ]
    
    note_elements = []
    for selector in selectors:
        note_elements = driver.find_elements(By.CSS_SELECTOR, selector)
        if note_elements:
            print(f"  âœ“ ä½¿ç”¨é€‰æ‹©å™¨: {selector}, æ‰¾åˆ° {len(note_elements)} ä¸ªç¬”è®°")
            break
    
    if not note_elements:
        print(f"  âœ— æœªæ‰¾åˆ°ä»»ä½•ç¬”è®°å…ƒç´ ")
        return pd.DataFrame()
    
    # æå–æ•°æ®
    for idx, elem in enumerate(note_elements[:max_posts]):
        try:
            # æå–ç¬”è®°é“¾æ¥ - ä¼˜å…ˆä»å…ƒç´ æœ¬èº«è·å–ï¼Œå¦åˆ™æŸ¥æ‰¾å­å…ƒç´ ä¸­çš„aæ ‡ç­¾
            link = elem.get_attribute("href") or ""
            
            # å¦‚æœå…ƒç´ æœ¬èº«æ²¡æœ‰hrefï¼Œå°è¯•åœ¨å†…éƒ¨æŸ¥æ‰¾aæ ‡ç­¾
            if not link:
                try:
                    link_elem = elem.find_element(By.CSS_SELECTOR, "a")
                    link = link_elem.get_attribute("href") or ""
                except NoSuchElementException:
                    pass
            
            # è°ƒè¯•è¾“å‡º
            if idx < 3:  # åªè¾“å‡ºå‰3ä¸ªå…ƒç´ çš„è°ƒè¯•ä¿¡æ¯
                print(f"  [è°ƒè¯•] å…ƒç´ {idx+1} link: {link[:80] if link else 'æ— é“¾æ¥'}")
            
            # å¦‚æœæ‰¾åˆ°é“¾æ¥ä¸”åŒ…å«å…³é”®è·¯å¾„ï¼Œæå–note_id
            note_id = ""
            if link and ("/explore/" in link or "/discovery/item/" in link):
                note_id = link.split("/")[-1].split("?")[0]
            else:
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆé“¾æ¥ï¼Œä»ç„¶å°è¯•æå–å…¶ä»–ä¿¡æ¯
                note_id = f"note_{idx+1}"
            
            # å°è¯•æå–æ ‡é¢˜
            title = ""
            try:
                # å°è¯•å¤šä¸ªå¯èƒ½çš„æ ‡é¢˜é€‰æ‹©å™¨
                title_selectors = [".title", ".note-title", "span.title", ".footer .title"]
                for sel in title_selectors:
                    try:
                        title_elem = elem.find_element(By.CSS_SELECTOR, sel)
                        title = title_elem.text.strip()
                        if title:
                            break
                    except NoSuchElementException:
                        continue
                
                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ ‡é¢˜ï¼Œè·å–æ•´ä¸ªå…ƒç´ çš„æ–‡æœ¬
                if not title:
                    title = elem.text.strip()[:100]  # å–å‰100å­—ç¬¦
                    
            except Exception:
                title = f"ç¬”è®°_{idx+1}"
            
            # å°è¯•æå–ä½œè€…
            user = ""
            try:
                user_selectors = [".author", ".username", ".name", ".author-wrapper .name"]
                for sel in user_selectors:
                    try:
                        user_elem = elem.find_element(By.CSS_SELECTOR, sel)
                        user = user_elem.text.strip()
                        if user:
                            break
                    except NoSuchElementException:
                        continue
            except Exception:
                user = "æœªçŸ¥"
            
            # å°è¯•æå–ç‚¹èµæ•°
            likes = 0
            try:
                like_selectors = [".like-count", ".likes", ".interaction .count", "span[class*='like']", ".footer-container .count"]
                for sel in like_selectors:
                    try:
                        like_elem = elem.find_element(By.CSS_SELECTOR, sel)
                        likes_text = like_elem.text.strip()
                        # å¤„ç†å¯èƒ½çš„kã€wç­‰å•ä½
                        if 'w' in likes_text:
                            likes = int(float(''.join(filter(lambda x: x.isdigit() or x == '.', likes_text))) * 10000)
                        elif 'k' in likes_text.lower():
                            likes = int(float(''.join(filter(lambda x: x.isdigit() or x == '.', likes_text))) * 1000)
                        else:
                            likes = int(''.join(filter(str.isdigit, likes_text))) if likes_text else 0
                        if likes > 0:
                            break
                    except (NoSuchElementException, ValueError):
                        continue
            except Exception:
                likes = 0
            
            # å°è¯•æå–è¯„è®ºæ•°
            comments = 0
            try:
                comment_selectors = [".comment-count", ".comments", "span[class*='comment']", ".footer-container .comment"]
                for sel in comment_selectors:
                    try:
                        comment_elem = elem.find_element(By.CSS_SELECTOR, sel)
                        comments_text = comment_elem.text.strip()
                        # å¤„ç†å¯èƒ½çš„kã€wç­‰å•ä½
                        if 'w' in comments_text:
                            comments = int(float(''.join(filter(lambda x: x.isdigit() or x == '.', comments_text))) * 10000)
                        elif 'k' in comments_text.lower():
                            comments = int(float(''.join(filter(lambda x: x.isdigit() or x == '.', comments_text))) * 1000)
                        else:
                            comments = int(''.join(filter(str.isdigit, comments_text))) if comments_text else 0
                        if comments > 0:
                            break
                    except (NoSuchElementException, ValueError):
                        continue
            except Exception:
                comments = 0
            
            # å°è¯•æå–å‘å¸ƒæ—¥æœŸ
            publish_date = ""
            try:
                date_selectors = [".publish-date", ".date", "span[class*='time']", ".footer-container .time"]
                for sel in date_selectors:
                    try:
                        date_elem = elem.find_element(By.CSS_SELECTOR, sel)
                        publish_date = date_elem.text.strip()
                        if publish_date:
                            break
                    except NoSuchElementException:
                        continue
            except Exception:
                publish_date = "æœªçŸ¥"
            
            # å°è¯•æå–è¯æ¡/æ ‡ç­¾
            tags = ""
            try:
                tag_selectors = [".tag", ".tags", "[class*='tag']", ".footer-container .tag"]
                tag_elements = []
                for sel in tag_selectors:
                    try:
                        tag_elements = elem.find_elements(By.CSS_SELECTOR, sel)
                        if tag_elements:
                            break
                    except NoSuchElementException:
                        continue
                
                if tag_elements:
                    tags = ", ".join([tag.text.strip() for tag in tag_elements[:5] if tag.text.strip()])  # æœ€å¤šå–5ä¸ªæ ‡ç­¾
            except Exception:
                tags = ""
            
            # åªè¦æœ‰æ ‡é¢˜å°±ä¿å­˜æ•°æ®ï¼ˆä¸å†å¼ºåˆ¶è¦æ±‚linkåŒ…å«exploreï¼‰
            if title and title != f"ç¬”è®°_{idx+1}":
                rows.append({
                    "ç¬”è®°ID": note_id,
                    "æ ‡é¢˜": title,
                    "ç”¨æˆ·": user if user else "æœªçŸ¥",
                    "å‘å¸ƒæ—¥æœŸ": publish_date if publish_date else "æœªçŸ¥",
                    "ç‚¹èµæ•°": likes,
                    "è¯„è®ºæ•°": comments,
                    "è¯æ¡/æ ‡ç­¾": tags if tags else "æ— ",
                    "é“¾æ¥": link if link else "æ— ",
                })
            
        except Exception as e:
            print(f"  âš ï¸  æå–ç¬¬ {idx+1} ä¸ªç¬”è®°æ•°æ®å¤±è´¥: {e}")
            continue
    
    print(f"  âœ“ æˆåŠŸæå– {len(rows)} æ¡ç¬”è®°æ•°æ®")
    return pd.DataFrame(rows)


def main():
    print("=" * 60)
    print("å°çº¢ä¹¦ç¬”è®°çˆ¬è™« (Seleniumç‰ˆ)")
    print("=" * 60)
    
    # åˆå§‹åŒ–æµè§ˆå™¨
    driver = init_driver()
    
    try:
        # ç™»å½•
        login_xiaohongshu(driver)
        
        # å¼€å§‹çˆ¬å–
        with pd.ExcelWriter("xiaohongshu_data.xlsx", engine="openpyxl") as writer:
            for keyword in KEYWORD_LIST:
                df = crawl_keyword(driver, keyword, MAX_POSTS)
                
                if df.empty:
                    print(f"  âš ï¸  å…³é”®è¯ '{keyword}' æ— æ•°æ®")
                    continue
                
                # ä¿å­˜åˆ°Excel
                sheet_name = keyword[:31]  # Sheetåé™åˆ¶31å­—ç¬¦
                df.to_excel(writer, sheet_name=sheet_name, index=False)
                print(f"  âœ… å…³é”®è¯ '{keyword}' å†™å…¥å®Œæˆï¼ˆ{len(df)} æ¡ï¼‰\n")
                
                # é—´éš”æ—¶é—´ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(random.uniform(2, 4))
        
        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ° xiaohongshu_data.xlsx")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
    finally:
        print("\nå…³é—­æµè§ˆå™¨...")
        driver.quit()


if __name__ == "__main__":
    main()
